/**
 * Servicio de API Gemini para JetAI
 * 
 * Este m√≥dulo proporciona endpoints para interactuar con la API Gemini (Generative Language)
 * de Google, permitiendo generar respuestas de texto inteligentes para la asistencia de viajes.
 */

import { Router, Request, Response } from 'express';
import { getGenerativeAIClient } from '../lib/googleApiConfig';

// Funci√≥n para generar respuesta usando Gemini
export const generateChatResponse = async (req: Request, res: Response) => {
  try {
    const { prompt, mode = 'travel-assistant', context = [] } = req.body;
    
    if (!prompt || typeof prompt !== 'string') {
      return res.status(400).json({
        error: 'Se requiere un prompt v√°lido',
        message: 'El campo "prompt" es obligatorio y debe ser una cadena de texto'
      });
    }
    
    const genAI = getGenerativeAIClient();
    if (!genAI) {
      return res.status(503).json({
        error: 'Servicio no disponible',
        message: 'El servicio de Gemini AI no est√° inicializado correctamente'
      });
    }
    
    console.log(`ü§ñ Generando respuesta de Gemini para: "${prompt.substring(0, 100)}..."`);
    
    // Preparar el prompt seg√∫n el modo
    let systemPrompt = '';
    
    switch (mode) {
      case 'travel-assistant':
        systemPrompt = `Eres JetAI, un asistente de viajes inteligente, emocional y multiling√ºe. Tu objetivo es ayudar a los usuarios a planificar viajes, encontrar destinos, reservar vuelos y hoteles, y proporcionar informaci√≥n √∫til para sus viajes. Responde de manera amable, √∫til y concisa.`;
        break;
      case 'luxury-concierge':
        systemPrompt = `Eres JetAI, un concierge de lujo para viajeros exclusivos. Ofreces recomendaciones premium, acceso VIP, y experiencias √∫nicas para viajeros exigentes. Tu tono es elegante, sofisticado y discreto. Anticipas las necesidades del cliente y ofreces soluciones a medida.`;
        break;
      case 'adventure-guide':
        systemPrompt = `Eres JetAI, un gu√≠a de aventuras experto. Especializ√°ndote en viajes de aventura y ecoturismo, ayudas a los viajeros a descubrir experiencias emocionantes, actividades al aire libre y destinos fuera de lo com√∫n. Tu tono es entusiasta, en√©rgico e inspirador.`;
        break;
      default:
        systemPrompt = `Eres JetAI, un asistente de viajes inteligente, emocional y multiling√ºe. Tu objetivo es ayudar a los usuarios a planificar viajes, encontrar destinos, reservar vuelos y hoteles, y proporcionar informaci√≥n √∫til para sus viajes. Responde de manera amable, √∫til y concisa.`;
    }
    
    // Intentar obtener el modelo Gemini utilizando la versi√≥n m√°s reciente
    // Nombres actualizados: models/gemini-1.5-flash-latest, models/gemini-1.5-pro-latest
    
    // Seleccionar modelo seg√∫n el modo - para casos que requieran imagen o an√°lisis m√°s profundo
    let modelName = "models/gemini-1.5-flash-latest"; // modelo r√°pido por defecto
    let requestedModel = modelName; // guardamos el modelo solicitado originalmente
    
    // Si se solicita un modo espec√≠fico que requiere an√°lisis de imagen o capacidades avanzadas
    if (mode === 'vision' || mode === 'analyze' || mode === 'multimodal') {
      requestedModel = "models/gemini-1.5-pro-latest"; // modelo m√°s poderoso con capacidades multimodales
    }
    
    // Lista priorizada de modelos a intentar (de m√°s avanzado a m√°s b√°sico)
    const modelPriority = [
      requestedModel,             // 1. El modelo solicitado (Pro o Flash seg√∫n el modo)
      "models/gemini-1.5-flash-latest", // 2. Flash como primera opci√≥n de fallback
      "gemini-pro",               // 3. Modelo antiguo como segunda opci√≥n
      "models/gemini-pro",        // 4. √öltima opci√≥n con nomenclatura alternativa
    ];
    
    // Filtrar los modelos duplicados para tener una lista √∫nica
    const uniqueModels: string[] = [];
    for (const model of modelPriority) {
      if (!uniqueModels.includes(model)) {
        uniqueModels.push(model);
      }
    }
    
    // Variable para almacenar el modelo una vez se inicialice con √©xito
    let model;
    let modelError: Error | null = null;
    
    // Intentar cada modelo hasta que uno funcione
    for (const candidateModel of uniqueModels) {
      try {
        console.log(`üîÑ Intentando inicializar modelo: ${candidateModel}`);
        model = genAI.getGenerativeModel({ model: candidateModel });
        modelName = candidateModel; // actualizar el nombre del modelo que se est√° usando
        console.log(`‚úÖ Modelo inicializado con √©xito: ${modelName}`);
        break; // Si tiene √©xito, salimos del bucle
      } catch (error: any) { // tipo 'any' para poder acceder a error.message
        console.log(`‚ö†Ô∏è Error al inicializar modelo ${candidateModel}: ${error.message || 'Error desconocido'}`);
        modelError = error;
        // Continuar con el siguiente modelo
      }
    }
    
    // Si ning√∫n modelo funcion√≥ despu√©s de probar todos
    if (!model) {
      throw new Error(`No se pudo inicializar ning√∫n modelo de Gemini: ${modelError?.message}`);
    }
    
    // Crear chat con contexto del sistema
    const chat = model.startChat({
      history: [
        {
          role: "user",
          parts: [{ text: "Por favor, act√∫a como un asistente de viajes llamado JetAI" }],
        },
        {
          role: "model",
          parts: [{ text: "¬°Hola! Soy JetAI, tu asistente personal de viajes. ¬øEn qu√© puedo ayudarte hoy?" }],
        },
        ...context.map((msg: any) => {
          // Mapear 'assistant' a 'model' para compatibilidad con la API de Gemini
          const role = msg.role === 'assistant' ? 'model' : (msg.role || 'user');
          return {
            role: role,
            parts: [{ text: msg.content || msg.text || "" }]
          };
        })
      ],
      generationConfig: {
        temperature: 0.7,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 1024,
      },
    });
    
    // Intentar enviar el mensaje con el modelo actual y fallback si falla por cuota
    let text: string;
    let usedModel = modelName;
    let attemptCount = 0;
    const maxAttempts = 3;
    
    // Lista de modelos para intentar en orden de preferencia si el actual falla
    const fallbackModels = [
      "models/gemini-1.5-flash-latest", // Modelo flash es m√°s permisivo con cuotas
      "gemini-pro",                    // Modelo antiguo como √∫ltimo recurso
    ].filter(m => m !== modelName); // Excluir el modelo actual
    
    try {
      // Primer intento con el modelo seleccionado
      console.log(`üîÑ Intentando generar respuesta con modelo: ${modelName}`);
      const result = await chat.sendMessage(prompt);
      const response = await result.response;
      text = response.text();
      console.log(`‚úÖ Respuesta generada exitosamente con ${modelName} (${text.length} caracteres)`);
    } catch (error: any) {
      console.log(`‚ö†Ô∏è Error al generar respuesta con modelo ${modelName}: ${error.message}`);
      
      // Si fall√≥ por exceder cuota, intentar con modelos de fallback
      if (error.status === 429 || error.message?.includes('quota') || error.message?.includes('rate limit')) {
        console.log('‚ö†Ô∏è Error de cuota o l√≠mite de velocidad, intentando con modelos alternativos...');
        
        // Intentar con cada modelo de fallback
        let fallbackSuccess = false;
        
        for (const fallbackModel of fallbackModels) {
          try {
            console.log(`üîÑ Intentando fallback con modelo: ${fallbackModel}`);
            
            // Inicializar nuevo modelo de fallback
            const fallbackModelInstance = genAI.getGenerativeModel({ model: fallbackModel });
            
            // Crear un nuevo chat con el modelo de fallback
            const fallbackChat = fallbackModelInstance.startChat({
              history: [
                {
                  role: "user",
                  parts: [{ text: "Por favor, act√∫a como un asistente de viajes llamado JetAI" }],
                },
                {
                  role: "model",
                  parts: [{ text: "¬°Hola! Soy JetAI, tu asistente personal de viajes. ¬øEn qu√© puedo ayudarte hoy?" }],
                },
                ...context.map((msg: any) => {
                  const role = msg.role === 'assistant' ? 'model' : (msg.role || 'user');
                  return {
                    role: role,
                    parts: [{ text: msg.content || msg.text || "" }]
                  };
                })
              ],
              generationConfig: {
                temperature: 0.7,
                topK: 40, 
                topP: 0.95,
                maxOutputTokens: 1024,
              },
            });
            
            // Enviar el mensaje con el modelo de fallback
            const fallbackResult = await fallbackChat.sendMessage(prompt);
            const fallbackResponse = await fallbackResult.response;
            text = fallbackResponse.text();
            
            // Si llegamos aqu√≠, funcion√≥ el fallback
            fallbackSuccess = true;
            usedModel = fallbackModel;
            console.log(`‚úÖ Respuesta generada exitosamente con modelo fallback ${fallbackModel} (${text.length} caracteres)`);
            break;
          } catch (fallbackError: any) {
            console.log(`‚ö†Ô∏è Error con modelo fallback ${fallbackModel}: ${fallbackError.message}`);
            // Continuar al siguiente modelo
          }
        }
        
        // Si ning√∫n fallback funcion√≥, lanza el error original
        if (!fallbackSuccess) {
          throw error;
        }
      } else {
        // Si el error no es de cuota, lanza el error original
        throw error;
      }
    }
    
    return res.status(200).json({
      text,
      model: usedModel,
      usage: {
        promptTokens: prompt.length / 4, // Estimaci√≥n aproximada
        completionTokens: text.length / 4, // Estimaci√≥n aproximada
        totalTokens: (prompt.length + text.length) / 4 // Estimaci√≥n aproximada
      }
    });
    
  } catch (error: any) {
    console.error('Error al generar respuesta de Gemini:', error);
    return res.status(500).json({
      error: 'Error en el servicio de Gemini',
      message: error.message || 'Error al procesar la solicitud',
      details: error.toString()
    });
  }
};

// Funci√≥n para verificar el estado del servicio Gemini
export const checkGeminiStatus = async (_req: Request, res: Response) => {
  try {
    const genAI = getGenerativeAIClient();
    
    if (!genAI) {
      return res.status(503).json({
        status: 'unavailable',
        message: 'El servicio de Gemini AI no est√° inicializado'
      });
    }
    
    // Intenta hacer una solicitud simple para verificar la conexi√≥n
    // Lista priorizada de modelos a intentar (de m√°s avanzado a m√°s b√°sico)
    const modelPriority = [
      "models/gemini-1.5-flash-latest", // 1. Flash como modelo principal por su eficiencia
      "models/gemini-1.5-pro-latest",   // 2. Pro como alternativa (puede fallar por cuota)
      "gemini-pro",                     // 3. Modelo antiguo
      "models/gemini-pro",              // 4. Nomenclatura alternativa
    ];
    
    // Variable para almacenar el modelo una vez se inicialice con √©xito
    let modelName = "";
    let model;
    let text = "";
    let modelError: Error | null = null;
    
    // Intentar cada modelo hasta que uno funcione
    for (const candidateModel of modelPriority) {
      try {
        console.log(`üîÑ Verificando disponibilidad del modelo: ${candidateModel}`);
        model = genAI.getGenerativeModel({ model: candidateModel });
        
        // Intentar generar contenido m√≠nimo para verificar
        const result = await model.generateContent("Hola");
        const response = await result.response;
        text = response.text();
        
        // Si llegamos aqu√≠, el modelo funciona correctamente
        modelName = candidateModel;
        console.log(`‚úÖ Modelo disponible y funcionando: ${modelName}`);
        break; // Si tiene √©xito, salimos del bucle
      } catch (error: any) {
        console.log(`‚ö†Ô∏è Error al verificar modelo ${candidateModel}: ${error.message || 'Error desconocido'}`);
        modelError = error;
        // Continuar con el siguiente modelo
      }
    }
    
    // Si ning√∫n modelo funcion√≥ despu√©s de probar todos
    if (!modelName) {
      throw new Error(`No se pudo verificar ning√∫n modelo de Gemini: ${modelError?.message}`);
    }
    
    return res.status(200).json({
      status: 'available',
      message: 'Servicio Gemini disponible y funcionando',
      model: modelName,
      sample: text
    });
    
  } catch (error: any) {
    console.error('Error al verificar el estado de Gemini:', error);
    return res.status(503).json({
      status: 'error',
      message: 'Error al conectar con el servicio Gemini',
      error: error.message || 'Error desconocido'
    });
  }
};

// Configuraci√≥n de rutas
export const configureRoutes = (app: any) => {
  const router = Router();
  
  // Rutas para el servicio Gemini
  router.post('/gemini/chat', generateChatResponse);
  router.get('/gemini/status', checkGeminiStatus);
  
  app.use('/api', router);
  
  console.log('‚úÖ Gemini service routes configured successfully');
};

export default {
  configureRoutes,
  generateChatResponse,
  checkGeminiStatus
};